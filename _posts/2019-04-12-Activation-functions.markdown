---
layout: post
title:  "Activation Functions: why we need it."
date:   2019-04-12 00:01:45 +0800
categories: yuehan notes
---

Why we need activation functions: if we only have linear neurons, then the network is no more than a linear regression function.

Therefore, we need activition.

Activation can also solve gradient vanishing and explosion.
