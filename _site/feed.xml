<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.8.5">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2019-04-30T18:03:34+08:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">John_date_river_side</title><subtitle>Hi, I'm Yuehan. I am putting some learning notes and project presentations here. Thanks for visiting! Drop me a message to my email address.</subtitle><entry><title type="html">Dive into graph embedding applications</title><link href="http://localhost:4000/yuehan/notes/2019/04/17/Dive-into-graph-embedding-applications.html" rel="alternate" type="text/html" title="Dive into graph embedding applications" /><published>2019-04-17T08:50:45+08:00</published><updated>2019-04-17T08:50:45+08:00</updated><id>http://localhost:4000/yuehan/notes/2019/04/17/Dive-into-graph-embedding-applications</id><content type="html" xml:base="http://localhost:4000/yuehan/notes/2019/04/17/Dive-into-graph-embedding-applications.html">&lt;ol&gt;
  &lt;li&gt;How to efficiently generate random paths in spark?&lt;/li&gt;
  &lt;li&gt;Why gradient explosion happends in spark.mllib.word2vec, but not in gensim.word2vec?&lt;/li&gt;
  &lt;li&gt;How to use map-reduce to compute degrees of each node in a giant graph(that have hundreds of thousands edges.) using spark-sql.&lt;/li&gt;
&lt;/ol&gt;</content><author><name></name></author><summary type="html">How to efficiently generate random paths in spark? Why gradient explosion happends in spark.mllib.word2vec, but not in gensim.word2vec? How to use map-reduce to compute degrees of each node in a giant graph(that have hundreds of thousands edges.) using spark-sql.</summary></entry><entry><title type="html">Compare l1 and l2 regularizaion.</title><link href="http://localhost:4000/yuehan/notes/2019/02/10/Compare-l1-and-l2-regularization.html" rel="alternate" type="text/html" title="Compare l1 and l2 regularizaion." /><published>2019-02-10T20:50:45+08:00</published><updated>2019-02-10T20:50:45+08:00</updated><id>http://localhost:4000/yuehan/notes/2019/02/10/Compare-l1-and-l2-regularization</id><content type="html" xml:base="http://localhost:4000/yuehan/notes/2019/02/10/Compare-l1-and-l2-regularization.html">&lt;p&gt;In this artical, we explain why l1 regularization causes sparse parameters, but l2 regularization can not.
Let’s suppose the cost functions of weights in terms of l1 and l2 regularization are&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;L_1(w) = J(w)+\sum_{i=1} |w_i|&lt;/script&gt;

&lt;p&gt;and&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;L_2(w) = J(w)+\sum_{i=1} (w_i)^2.&lt;/script&gt;

&lt;p&gt;We apply gradient descent to update $w_i$, which means&lt;/p&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;w_i =  w_i-\alpha\cdot\frac{\delta L_1}{\delta w_i}\\
	= w_i-\frac{\delta J(w)}{\delta w}-sgn(w_i)&lt;/script&gt; for l1 reg,&lt;/p&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;w_i =  w_i-\alpha\cdot\frac{\delta L_1}{\delta w_i}\\
	= w_i-\frac{\delta J(w)}{\delta w}-2(w_i)&lt;/script&gt;for l2 reg.&lt;/p&gt;

&lt;p&gt;Note that sgn represents signal function. Above shows that when $w\in(1,+\infty)$, $w_i$ of l2 decreases faster than that og l1 reg. When $w$ becomes less than 1 and approximates 0, $w_i$ for l2 decreases extremely slowly, while $w_i$ for l1 maintains a constant decreasing velocity. Thus if we keep training under l1 reg, many of $w$ will become zero ultimately.&lt;/p&gt;</content><author><name></name></author><summary type="html">In this artical, we explain why l1 regularization causes sparse parameters, but l2 regularization can not. Let’s suppose the cost functions of weights in terms of l1 and l2 regularization are</summary></entry><entry><title type="html">Activation Functions: why we need it.</title><link href="http://localhost:4000/yuehan/notes/2019/01/21/Activation-functions.html" rel="alternate" type="text/html" title="Activation Functions: why we need it." /><published>2019-01-21T00:01:45+08:00</published><updated>2019-01-21T00:01:45+08:00</updated><id>http://localhost:4000/yuehan/notes/2019/01/21/Activation-functions</id><content type="html" xml:base="http://localhost:4000/yuehan/notes/2019/01/21/Activation-functions.html">&lt;p&gt;Why we need activation functions: if we only have linear neurons, then the network is no more than a linear regression function.&lt;/p&gt;

&lt;p&gt;Therefore, we need activition.&lt;/p&gt;

&lt;p&gt;Activation can also solve gradient vanishing and explosion.&lt;/p&gt;</content><author><name></name></author><summary type="html">Why we need activation functions: if we only have linear neurons, then the network is no more than a linear regression function.</summary></entry><entry><title type="html">Compute gradients in neural network.</title><link href="http://localhost:4000/yuehan/notes/2019/01/03/Gradients-in-neural-network.html" rel="alternate" type="text/html" title="Compute gradients in neural network." /><published>2019-01-03T13:57:45+08:00</published><updated>2019-01-03T13:57:45+08:00</updated><id>http://localhost:4000/yuehan/notes/2019/01/03/Gradients-in-neural-network</id><content type="html" xml:base="http://localhost:4000/yuehan/notes/2019/01/03/Gradients-in-neural-network.html">&lt;p&gt;How neural network update weights of its hidden units? This blog mainly refers from Andrew Ng’s course &lt;a href=&quot;https://www.coursera.org/learn/neural-networks-deep-learning/home/week/3&quot;&gt;Deep learning&lt;/a&gt;.&lt;/p&gt;
&lt;h1 id=&quot;do-the-prediction&quot;&gt;Do the prediction&lt;/h1&gt;
&lt;p&gt;Suppose we are constructing the simplest neural network that only has 1 layer of hidden neurons. It can be regarded as a logistic regression model. For one sample, given weights, we first compute the predicted $\bar{y}$, and the logloss $L$ caused by error between it and ground true:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;z=w_1x_1+w_2x_2+b\\
\bar{y}=\sigma(z) = \frac{1}{1+e^{-z}}\\
L(\bar{y},y)=(1-y)log(\frac{1}{1-\bar{y}})+ylog(\frac{1}{\bar{y}})&lt;/script&gt;

&lt;h1 id=&quot;compute-gradients-to-update-weights&quot;&gt;Compute gradients to update weights&lt;/h1&gt;
&lt;p&gt;Hereto, $L$ shows the perforance of $w_1, w_2, b$, so we updates those weights(parameters) accordingly. We use gradient descent to update those parameters. Here is a general form of gradient descent, taking updating $w_1$ for instance:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;w_{1,t+1}=w_{1,t}-\alpha \frac{\delta L}{\delta w_{1,t}},&lt;/script&gt;

&lt;p&gt;The $\alpha$ here is learning rate. So we have to compute $\frac{\delta L}{\delta w_1,t}$, we simply note it $\delta w_1$.
According to chain rule,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\delta w_1=x_1\cdot \frac{\delta L}{\delta z}\\
=x_1\cdot\frac{\delta L}{\delta \bar{y}}\cdot \frac{\delta \bar{y}}{\delta z}\\
=x_1\cdot (y-\bar{y})&lt;/script&gt;

&lt;p&gt;When we have $m$ examples, we just sum them up to obtain $\frac{1}{m}\sum_{i=1}^m\delta w_1^{(i)}$, because $L$ summing up across all samples is additive.&lt;/p&gt;

&lt;h1 id=&quot;back-propagation&quot;&gt;Back propagation&lt;/h1&gt;
&lt;p&gt;In the previous example, we only have one layer of weights to update. In a deeper network, the formula of $\delta w_1$ will incorptes weights of latter layers. In this case, we first update those latter weights (in terms of layer index), then update $w_1$, following the same methodology. This procedure is called back propagation.&lt;/p&gt;

&lt;p&gt;Back propagation leads to two problems regarding gradients. They are &lt;code class=&quot;highlighter-rouge&quot;&gt;gradient vanishing&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;gradient explosion&lt;/code&gt;. We will talk about using different activation funciton to solve those two problems in another blog.&lt;/p&gt;</content><author><name></name></author><summary type="html">How neural network update weights of its hidden units? This blog mainly refers from Andrew Ng’s course Deep learning. Do the prediction Suppose we are constructing the simplest neural network that only has 1 layer of hidden neurons. It can be regarded as a logistic regression model. For one sample, given weights, we first compute the predicted $\bar{y}$, and the logloss $L$ caused by error between it and ground true:</summary></entry></feed>