<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.6.0 -->
<title>Compute gradients in neural network. | John_date_river_side</title>
<meta name="generator" content="Jekyll v3.8.5" />
<meta property="og:title" content="Compute gradients in neural network." />
<meta property="og:locale" content="en_US" />
<meta name="description" content="How neural network update weights of its hidden units? This blog mainly refers from Andrew Ng’s course Deep learning. Do the prediction Suppose we are constructing the simplest neural network that only has 1 layer of hidden neurons. It can be regarded as a logistic regression model. For one sample, given weights, we first compute the predicted $\bar{y}$, and the logloss $L$ caused by error between it and ground true:" />
<meta property="og:description" content="How neural network update weights of its hidden units? This blog mainly refers from Andrew Ng’s course Deep learning. Do the prediction Suppose we are constructing the simplest neural network that only has 1 layer of hidden neurons. It can be regarded as a logistic regression model. For one sample, given weights, we first compute the predicted $\bar{y}$, and the logloss $L$ caused by error between it and ground true:" />
<link rel="canonical" href="http://localhost:4000/yuehan/notes/2019/01/03/Gradients-in-neural-network.html" />
<meta property="og:url" content="http://localhost:4000/yuehan/notes/2019/01/03/Gradients-in-neural-network.html" />
<meta property="og:site_name" content="John_date_river_side" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2019-01-03T13:57:45+08:00" />
<script type="application/ld+json">
{"description":"How neural network update weights of its hidden units? This blog mainly refers from Andrew Ng’s course Deep learning. Do the prediction Suppose we are constructing the simplest neural network that only has 1 layer of hidden neurons. It can be regarded as a logistic regression model. For one sample, given weights, we first compute the predicted $\\bar{y}$, and the logloss $L$ caused by error between it and ground true:","@type":"BlogPosting","url":"http://localhost:4000/yuehan/notes/2019/01/03/Gradients-in-neural-network.html","headline":"Compute gradients in neural network.","dateModified":"2019-01-03T13:57:45+08:00","datePublished":"2019-01-03T13:57:45+08:00","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/yuehan/notes/2019/01/03/Gradients-in-neural-network.html"},"@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="http://localhost:4000/feed.xml" title="John_date_river_side" /><script type="text/x-mathjax-config"> 
   		MathJax.Hub.Config({ TeX: { equationNumbers: { autoNumber: "all" } } }); 
   	</script>
    <script type="text/x-mathjax-config">
    	MathJax.Hub.Config({tex2jax: {
             inlineMath: [ ['$','$'], ["\\(","\\)"] ],
             processEscapes: true
           }
         });
    </script>
    
    <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript">
    </script>
</head><body><header class="site-header" role="banner">

  <div class="wrapper"><a class="site-title" rel="author" href="/">John_date_river_side</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/about/">About</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Compute gradients in neural network.</h1>
    <p class="post-meta">
      <time class="dt-published" datetime="2019-01-03T13:57:45+08:00" itemprop="datePublished">Jan 3, 2019
      </time></p>
  </header>

  <div class="post-content e-content" itemprop="articleBody">
    <p>How neural network update weights of its hidden units? This blog mainly refers from Andrew Ng’s course <a href="https://www.coursera.org/learn/neural-networks-deep-learning/home/week/3">Deep learning</a>.</p>
<h1 id="do-the-prediction">Do the prediction</h1>
<p>Suppose we are constructing the simplest neural network that only has 1 layer of hidden neurons. It can be regarded as a logistic regression model. For one sample, given weights, we first compute the predicted $\bar{y}$, and the logloss $L$ caused by error between it and ground true:</p>

<script type="math/tex; mode=display">z=w_1x_1+w_2x_2+b\\
\bar{y}=\sigma(z) = \frac{1}{1+e^{-z}}\\
L(\bar{y},y)=(1-y)log(\frac{1}{1-\bar{y}})+ylog(\frac{1}{\bar{y}})</script>

<h1 id="compute-gradients-to-update-weights">Compute gradients to update weights</h1>
<p>Hereto, $L$ shows the perforance of $w_1, w_2, b$, so we updates those weights(parameters) accordingly. We use gradient descent to update those parameters. Here is a general form of gradient descent, taking updating $w_1$ for instance:</p>

<script type="math/tex; mode=display">w_{1,t+1}=w_{1,t}-\alpha \frac{\delta L}{\delta w_{1,t}},</script>

<p>The $\alpha$ here is learning rate. So we have to compute $\frac{\delta L}{\delta w_1,t}$, we simply note it $\delta w_1$.
According to chain rule,</p>

<script type="math/tex; mode=display">\delta w_1=x_1\cdot \frac{\delta L}{\delta z}\\
=x_1\cdot\frac{\delta L}{\delta \bar{y}}\cdot \frac{\delta \bar{y}}{\delta z}\\
=x_1\cdot (y-\bar{y})</script>

<p>When we have $m$ examples, we just sum them up to obtain $\frac{1}{m}\sum_{i=1}^m\delta w_1^{(i)}$, because $L$ summing up across all samples is additive.</p>

<h1 id="back-propagation">Back propagation</h1>
<p>In the previous example, we only have one layer of weights to update. In a deeper network, the formula of $\delta w_1$ will incorptes weights of latter layers. In this case, we first update those latter weights (in terms of layer index), then update $w_1$, following the same methodology. This procedure is called back propagation.</p>

<p>Back propagation leads to two problems regarding gradients. They are <code class="highlighter-rouge">gradient vanishing</code> and <code class="highlighter-rouge">gradient explosion</code>. We will talk about using different activation funciton to solve those two problems in another blog.</p>


  </div><a class="u-url" href="/yuehan/notes/2019/01/03/Gradients-in-neural-network.html" hidden></a>
</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <h2 class="footer-heading">John_date_river_side</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li class="p-name">John_date_river_side</li><li><a class="u-email" href="mailto:steflyu@163.com">steflyu@163.com</a></li></ul>
      </div>

      <div class="footer-col footer-col-2"><ul class="social-media-list"><li><a href="https://github.com/yuehanlyu"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#github"></use></svg> <span class="username">yuehanlyu</span></a></li><li><a href="https://www.twitter.com/yuehanlyu"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#twitter"></use></svg> <span class="username">yuehanlyu</span></a></li></ul>
</div>

      <div class="footer-col footer-col-3">
        <p>Hi, I&#39;m Yuehan. I am putting some learning notes and project presentations here. Thanks for visiting! Drop me a message to my email address.</p>
      </div>
    </div>

  </div>

</footer>
</body>

</html>
